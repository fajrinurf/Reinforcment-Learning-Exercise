{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning: Pengenalan**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di dalam Tutorial ini, akan menjelaskan penggunaan Reinforcement Learning dasar yang akan digunakan, dan juga untuk memenuhi tugas kuliah Advance Machine Learning. Referensi yang akan digunakan di dalam tutorial ini akan berbasis dari buku dan juga paper. untuk kasus yang akan dijelaskan disini adalah penggunaan Reinforcement Learning yang akan menerapkan metode Q-learning tabular klasik untuk [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) klasik Puzzle. \n",
    "\n",
    "![alt text](https://media2.giphy.com/media/46ib09ZL1SdWuREnj3/giphy.gif?cid=3640f6095c6e92762f3446634d90bc65) ![alt text](https://media0.giphy.com/media/d9QiBcfzg64Io/200w.webp?cid=3640f6095c6e93e92f30655873731752)![alt text](https://i.gifer.com/GpAY.gif)\n",
    "\n",
    "Reinforcement Learning bisa beroperasi dengan melakukan indentifikasi pola yang akan digunakan secara optimal, di dalam konteks dari masalah masalah yang diberikan, sehingga agen pada reinforcement learning dapat membuat keputusan terbaik untuk langkah berikutnya."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Q-Learning with Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma:  0.8\n",
      "Value iteration policy: [1 3 0 3 0 0 0 0 3 1 1 1 0 2 2 0]\n",
      "\n",
      "\n",
      "policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "V: 1\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333]\n",
      " [0.         0.         0.         0.        ]]\n",
      "V: 2\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "policy: [0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0]\n",
      "\n",
      "V: 3\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333]\n",
      " [0.         0.         0.33333333 0.        ]]\n",
      "V: 4\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.17777778 0.46962963]\n",
      " [0.         0.         0.42222222 0.        ]]\n",
      "V: 5\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.04740741 0.        ]\n",
      " [0.         0.         0.23782716 0.52198848]\n",
      " [0.         0.         0.44592593 0.        ]]\n",
      "V: 6\n",
      "[[0.         0.         0.01264198 0.00337119]\n",
      " [0.         0.         0.06679177 0.        ]\n",
      " [0.         0.         0.25811051 0.54135973]\n",
      " [0.         0.         0.45224691 0.        ]]\n",
      "V: 7\n",
      "[[0.         0.         0.02118233 0.00654761]\n",
      " [0.         0.         0.07447809 0.        ]\n",
      " [0.         0.         0.26496177 0.5483524 ]\n",
      " [0.         0.         0.45393251 0.        ]]\n",
      "V: 8\n",
      "[[0.         0.         0.02550945 0.00854855]\n",
      " [0.         0.         0.07745899 0.        ]\n",
      " [0.         0.         0.26727598 0.55083423]\n",
      " [0.         0.         0.454382   0.        ]]\n",
      "V: 9\n",
      "[[0.         0.         0.02745825 0.00960181]\n",
      " [0.         0.         0.07859579 0.        ]\n",
      " [0.         0.         0.26805766 0.55170451]\n",
      " [0.         0.         0.45450187 0.        ]]\n",
      "V: 10\n",
      "[[0.         0.         0.02828108 0.0101021 ]\n",
      " [0.         0.         0.07902366 0.        ]\n",
      " [0.         0.         0.2683217  0.55200699]\n",
      " [0.         0.         0.45453383 0.        ]]\n",
      "\n",
      "policy: [0 1 2 3 0 0 0 0 0 1 2 1 0 1 2 0]\n",
      "\n",
      "V: 11\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333]\n",
      " [0.         0.         0.33333333 0.        ]]\n",
      "V: 12\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.17777778 0.46962963]\n",
      " [0.         0.08888889 0.46962963 0.        ]]\n",
      "V: 13\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.04740741 0.        ]\n",
      " [0.         0.07111111 0.26311111 0.52873086]\n",
      " [0.         0.14893827 0.52873086 0.        ]]\n",
      "V: 14\n",
      "[[0.         0.         0.01264198 0.00337119]\n",
      " [0.         0.         0.07353416 0.        ]\n",
      " [0.         0.10987984 0.3015989  0.5547546 ]\n",
      " [0.         0.18071177 0.5547546  0.        ]]\n",
      "V: 15\n",
      "[[0.         0.00337119 0.02387929 0.00816578]\n",
      " [0.         0.         0.08679418 0.        ]\n",
      " [0.         0.12861618 0.31901424 0.56633836]\n",
      " [0.         0.19612437 0.56633836 0.        ]]\n",
      "V: 16\n",
      "[[0.         0.00636781 0.03169047 0.01280587]\n",
      " [0.         0.         0.09352125 0.        ]\n",
      " [0.         0.13737029 0.32698613 0.5715532 ]\n",
      " [0.         0.20332339 0.5715532  0.        ]]\n",
      "V: 17\n",
      "[[0.         0.00845079 0.03680469 0.01664438]\n",
      " [0.         0.         0.09701088 0.        ]\n",
      " [0.         0.14141587 0.33069794 0.57393364]\n",
      " [0.         0.20663376 0.57393364 0.        ]]\n",
      "V: 18\n",
      "[[0.         0.00981458 0.04012266 0.01957638]\n",
      " [0.         0.         0.09888549 0.        ]\n",
      " [0.         0.14328845 0.3324674  0.57504028]\n",
      " [0.         0.2081513  0.57504028 0.        ]]\n",
      "V: 19\n",
      "[[0.         0.01069937 0.04228921 0.02171786]\n",
      " [0.         0.         0.0999351  0.        ]\n",
      " [0.         0.14416499 0.33333751 0.57556741]\n",
      " [0.         0.20885109 0.57556741 0.        ]]\n",
      "V: 20\n",
      "[[0.         0.01127712 0.04371791 0.02324097]\n",
      " [0.         0.         0.10054811 0.        ]\n",
      " [0.         0.14458363 0.33378211 0.57582654]\n",
      " [0.         0.20917827 0.57582654 0.        ]]\n",
      "V: 21\n",
      "[[0.         0.01165811 0.04466853 0.02430679]\n",
      " [0.         0.         0.10092017 0.        ]\n",
      " [0.         0.14478943 0.33401953 0.57595895]\n",
      " [0.         0.20933461 0.57595895 0.        ]]\n",
      "V: 22\n",
      "[[0.         0.01191161 0.04530546 0.02504508]\n",
      " [0.         0.         0.10115333 0.        ]\n",
      " [0.         0.14489444 0.33415233 0.57602968]\n",
      " [0.         0.20941162 0.57602968 0.        ]]\n",
      "\n",
      "policy: [1 2 2 3 0 0 0 0 1 1 1 1 0 2 2 0]\n",
      "\n",
      "V: 23\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333]\n",
      " [0.         0.         0.33333333 0.        ]]\n",
      "V: 24\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.17777778 0.46962963]\n",
      " [0.         0.08888889 0.46962963 0.        ]]\n",
      "V: 25\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.04740741 0.        ]\n",
      " [0.         0.07111111 0.2694321  0.53041646]\n",
      " [0.         0.16790123 0.53041646 0.        ]]\n",
      "V: 26\n",
      "[[0.         0.         0.01264198 0.00337119]\n",
      " [0.         0.         0.07521975 0.        ]\n",
      " [0.01896296 0.12167901 0.31533652 0.55886746]\n",
      " [0.         0.21866579 0.55886746 0.        ]]\n",
      "V: 27\n",
      "[[0.         0.00337119 0.02432878 0.00828564]\n",
      " [0.00505679 0.         0.09057741 0.        ]\n",
      " [0.03750453 0.15240182 0.33870313 0.57268549]\n",
      " [0.         0.24798269 0.57268549 0.        ]]\n",
      "V: 28\n",
      "[[0.00224746 0.00738666 0.03285116 0.01317932]\n",
      " [0.01194901 0.         0.09908114 0.        ]\n",
      " [0.05064169 0.169954   0.35075333 0.57958369]\n",
      " [0.         0.26416591 0.57958369 0.        ]]\n",
      "V: 29\n",
      "[[0.0057555  0.01073008 0.03869643 0.01734802]\n",
      " [0.01822565 0.         0.10385327 0.        ]\n",
      " [0.05882552 0.17966527 0.35702204 0.58309486]\n",
      " [0.         0.27291063 0.58309486 0.        ]]\n",
      "V: 30\n",
      "[[0.00925633 0.0131804  0.04263939 0.02062278]\n",
      " [0.02301533 0.         0.10657638 0.        ]\n",
      " [0.06359754 0.18494139 0.36030163 0.58490573]\n",
      " [0.         0.27758584 0.58490573 0.        ]]\n",
      "V: 31\n",
      "[[0.01212055 0.01488528 0.04529028 0.02307622]\n",
      " [0.02632891 0.         0.10815784 0.        ]\n",
      " [0.06627705 0.1877772  0.36202364 0.58584783]\n",
      " [0.         0.28007167 0.58584783 0.        ]]\n",
      "V: 32\n",
      "[[0.0142226  0.01604682 0.04707316 0.02486016]\n",
      " [0.02848762 0.         0.10909248 0.        ]\n",
      " [0.0677478  0.1892915  0.36292991 0.58634073]\n",
      " [0.         0.2813896  0.58634073 0.        ]]\n",
      "V: 33\n",
      "[[0.01566854 0.01683199 0.04827355 0.0261317 ]\n",
      " [0.02984106 0.         0.10965426 0.        ]\n",
      " [0.06854381 0.19009689 0.36340756 0.58659954]\n",
      " [0.         0.28208726 0.58659954 0.        ]]\n",
      "V: 34\n",
      "[[0.01662442 0.01736148 0.04908253 0.02702558]\n",
      " [0.03066914 0.         0.10999736 0.        ]\n",
      " [0.06897085 0.19052418 0.36365954 0.58673576]\n",
      " [0.         0.28245626 0.58673576 0.        ]]\n",
      "\n",
      "policy: [1 3 2 3 0 0 0 0 3 1 1 1 0 2 2 0]\n",
      "\n",
      "V: 35\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333]\n",
      " [0.         0.         0.33333333 0.        ]]\n",
      "V: 36\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.17777778 0.46962963]\n",
      " [0.         0.08888889 0.46962963 0.        ]]\n",
      "V: 37\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.04740741 0.        ]\n",
      " [0.         0.07111111 0.2694321  0.53041646]\n",
      " [0.         0.16790123 0.53041646 0.        ]]\n",
      "V: 38\n",
      "[[0.         0.         0.01264198 0.00337119]\n",
      " [0.         0.         0.07521975 0.        ]\n",
      " [0.01896296 0.12167901 0.31533652 0.55886746]\n",
      " [0.         0.21866579 0.55886746 0.        ]]\n",
      "V: 39\n",
      "[[0.         0.00337119 0.02432878 0.00828564]\n",
      " [0.00505679 0.         0.09057741 0.        ]\n",
      " [0.038853   0.15276142 0.33879902 0.57271106]\n",
      " [0.         0.24807858 0.57271106 0.        ]]\n",
      "V: 40\n",
      "[[0.00224746 0.00798598 0.03285116 0.01317932]\n",
      " [0.0123086  0.         0.09910671 0.        ]\n",
      " [0.05437947 0.17100189 0.3510464  0.57966866]\n",
      " [0.         0.26447774 0.57966866 0.        ]]\n",
      "V: 41\n",
      "[[0.00601121 0.01249289 0.03870325 0.01734984]\n",
      " [0.01938648 0.         0.10393324 0.        ]\n",
      " [0.06527142 0.18154548 0.35756875 0.58326331]\n",
      " [0.         0.27351784 0.58326331 0.        ]]\n",
      "V: 42\n",
      "[[0.01010416 0.01634675 0.04266302 0.02063005]\n",
      " [0.02526988 0.         0.10672847 0.        ]\n",
      " [0.07255648 0.18763815 0.3611106  0.58516638]\n",
      " [0.         0.27851181 0.58516638 0.        ]]\n",
      "V: 43\n",
      "[[0.01379221 0.01941386 0.04533908 0.02309312]\n",
      " [0.02976495 0.         0.10838658 0.        ]\n",
      " [0.07732255 0.19118532 0.36307149 0.58619676]\n",
      " [0.         0.28129694 0.58619676 0.        ]]\n",
      "V: 44\n",
      "[[0.01679227 0.02174539 0.04715167 0.02489011]\n",
      " [0.03303461 0.         0.10939284 0.        ]\n",
      " [0.08041133 0.1932746  0.36417817 0.58676665]\n",
      " [0.         0.28287155 0.58676665 0.        ]]\n",
      "V: 45\n",
      "[[0.01908594 0.02346213 0.04838257 0.02617674]\n",
      " [0.03534183 0.         0.1100162  0.        ]\n",
      " [0.0824074  0.1945219  0.36481472 0.58708836]\n",
      " [0.         0.28377602 0.58708836 0.        ]]\n",
      "V: 46\n",
      "[[0.02077064 0.02469742 0.04922013 0.0270863 ]\n",
      " [0.03693863 0.         0.11040929 0.        ]\n",
      " [0.08369812 0.19527703 0.36518767 0.58727361]\n",
      " [0.         0.28430438 0.58727361 0.        ]]\n",
      "V: 47\n",
      "[[0.02197512 0.02557138 0.04979086 0.02772359]\n",
      " [0.03802983 0.         0.11066094 0.        ]\n",
      " [0.08453466 0.19574046 0.36541005 0.58738231]\n",
      " [0.         0.28461825 0.58738231 0.        ]]\n",
      "V: 48\n",
      "[[0.02282036 0.02618203 0.0501801  0.02816727]\n",
      " [0.03876929 0.         0.11082404 0.        ]\n",
      " [0.08507851 0.19602848 0.36554483 0.58744724]\n",
      " [0.         0.28480774 0.58744724 0.        ]]\n",
      "\n",
      "Policy iteration policy: [1 3 2 3 0 0 0 0 3 1 1 1 0 2 2 0]\n",
      "\n",
      "------------------------\n",
      "END\n",
      "------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# You need this part\n",
    "# S:Start, F:Frozen, H:Hole, G:Goal\n",
    "map = [\"SFFF\", \"FHFH\", \"FFFF\", \"HFFG\"]\n",
    "# is_slippery=True means stochastic and is_slippery=False means deterministic\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"human\", desc=map, map_name=\"4x4\", is_slippery=True)\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "# You need to find the policy using both value iteration and policy iteration\n",
    "# You may not need this part!\n",
    "action = [\"left\", \"down\", \"right\", \"up\"]\n",
    "ncols = 4\n",
    "nrows = 4\n",
    "e = 0.001\n",
    "max_iterations = 1000 #  maximum iterations if there is an infinite loop\n",
    "\n",
    "# GIVEN\n",
    "# A sample policy to make the following while loop works\n",
    "# policy = [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 1, 0, 2, 2, 0]\n",
    "\n",
    "#  Initializing the variables\n",
    "n_states = env.observation_space.n # the total number of states in the environment\n",
    "n_actions = env.action_space.n # number of possible actions in the environment\n",
    "gamma_list = [0.8] # substitute with ('0.5' and '1')\n",
    "\n",
    "'''\n",
    "This function implements the value iteration algorithm to compute the optimal policy.\n",
    "\n",
    "It initializes the state value function (V) for all states to 0 and then iteratively updates the values until they converge to the optimal values.\n",
    "The loop continues until the maximum change in any value is less than the error threshold (e).\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "env : an object of a Gym environment class\n",
    "    Given environment\n",
    "\n",
    "gamma : float\n",
    "    Discount factor\n",
    "\n",
    "e : float\n",
    "    Error threshold\n",
    "\n",
    "Returns\n",
    "-------\n",
    "policy\n",
    "    1-D NumPy array of integers\n",
    "    Each element in the array represents the best action to take in the corresponding state to maximize the expected cumulative reward\n",
    "    The optimal policy\n",
    "\n",
    "'''\n",
    "def value_iteration(env, gamma, e):\n",
    "    V = np.zeros(n_states)\n",
    "\n",
    "    # check for convergence\n",
    "    # runs until 'delta' is less than a predefined value 'e'\n",
    "    while True:\n",
    "        delta = 0   # the maximum absolute difference between the old value of a state v and the new value V[s] computed in the current iteration\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            q_vals = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                for p, s_next, r, done in env.P[s][a]:\n",
    "                    q_vals[a] += p * (r + gamma * V[s_next])\n",
    "                V[s] = max(q_vals)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < e:\n",
    "            break\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    for s in range(n_states):\n",
    "        q_vals = np.zeros(n_actions)\n",
    "        for a in range(n_actions):\n",
    "            for p, s_next, r, done in env.P[s][a]:\n",
    "                q_vals[a] += p * (r + gamma * V[s_next])              \n",
    "            policy[s] = np.argmax(q_vals)\n",
    "    return policy\n",
    "\n",
    "'''\n",
    "This function implements the policy iteration algorithm to compute the optimal policy.\n",
    "\n",
    "It initializes a random policy for all states, then iteratively evaluates and improves the policy until convergence.\n",
    "In the evaluation step, it computes the state values for the given policy until they converge to the optimal values.\n",
    "In the improvement step, it updates the policy for each state by selecting the action that maximizes the expected value of the next state.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "env : an object of a Gym environment class\n",
    "    Given environment\n",
    "\n",
    "gamma : float\n",
    "    Discount factor\n",
    "\n",
    "e : float\n",
    "    Error threshold\n",
    "\n",
    "Returns\n",
    "-------\n",
    "policy\n",
    "    1-D NumPy array of integers\n",
    "    Each element in the array represents the best action to take in the corresponding state to maximize the expected cumulative reward.\n",
    "    The optimal policy.\n",
    "\n",
    "'''\n",
    "def policy_iteration(env, gamma, e):\n",
    "    num = 0 # to check for the iterations\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    print()\n",
    "    print(\"policy:\", policy) # Add print statement to see policy at each step\n",
    "    print() \n",
    "    while True:\n",
    "        V = np.zeros(n_states)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            #if num > max_iterations:   # stops iteration at 1000 but the GUI hangs\n",
    "            #    break                \n",
    "            for s in range(n_states):\n",
    "                v = V[s]\n",
    "                a = policy[s]\n",
    "                q_val = 0\n",
    "                for p, s_next, r, done in env.P[s][a]:\n",
    "                    q_val += p * (r + gamma * V[s_next])\n",
    "                V[s] = q_val\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            num += 1 # increment to keep count\n",
    "            print(\"V: \" + str(num))\n",
    "            print(V.reshape(4, 4)) # Add print statement to see V values at each step\n",
    "            if delta < e:\n",
    "                break\n",
    "        policy_stable = True\n",
    "\n",
    "        # check for convergence\n",
    "        # old policy at each state is saved in 'old_action', and then the Q-values for each action at the state are evaluated using the updated \n",
    "        # value function 'V'. The policy is then updated to choose the action with the highest Q-value, and if the updated policy at any state \n",
    "        # is different from the old policy, then policy_stable is set to False.\n",
    "        for s in range(n_states):\n",
    "            old_action = policy[s]\n",
    "            q_vals = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                for p, s_next, r, done in env.P[s][a]:\n",
    "                    q_vals[a] += p * (r + gamma * V[s_next])\n",
    "            policy[s] = np.argmax(q_vals)\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "        if policy_stable: # if 'policy_stable' remains True after the loop, then the policy is considered to have converged and the iteration loop is broken\n",
    "            break\n",
    "        print()\n",
    "        print(\"policy:\", policy) # Add print statement to see policy at each step\n",
    "        print()  \n",
    "    return policy\n",
    "\n",
    "# Loop to print out the optimal policies:\n",
    "# Both policies are represented as arrays of integers, with each index corresponding to a state in the environment and the value at that \n",
    "# index representing the action to be taken in that state according to the optimal policy\n",
    "for gamma in gamma_list:\n",
    "    print(\"gamma: \", gamma)\n",
    "    value_policy = value_iteration(env, gamma, e) #the optimal policy obtained by running the value iteration algorithm on the environment\n",
    "    print(\"Value iteration policy:\", value_policy)\n",
    "    print()\n",
    "    policy_policy = policy_iteration(env, gamma, e) #the optimal policy obtained by running the policy iteration algorithm on the environmeny\n",
    "    print() \n",
    "    print(\"Policy iteration policy:\", policy_policy)\n",
    "    print()\n",
    "    print(\"------------------------\")\n",
    "\n",
    "\n",
    "# GIVEN\n",
    "# This part uses the found policy to interact with the environment.\n",
    "# You don't need to change anything here.\n",
    "\n",
    "s = 0\n",
    "goal = ncols * nrows - 1\n",
    "while s != goal:\n",
    "    a = value_policy[s]\n",
    "    s, r, t, f, p = env.step(a)\n",
    "    if t == True and s != goal:\n",
    "        env.reset()\n",
    "        s = 0\n",
    "print(\"END\")\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, numpy as np, matplotlib.pyplot as plt\n",
    "from neural_networks.policy_gradient_utilities import PolicyGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 5\n",
    "gamma = .99\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "n_episodes = 10000\n",
    "render = False\n",
    "goal = 190\n",
    "n_layers = 2\n",
    "n_classes = 2\n",
    "environment = gym.make('CartPole-v1')\n",
    "environment_dimension = len(environment.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_reward(reward, gamma=gamma):\n",
    "    output = [reward[i] * gamma**i for i in range(0, len(reward))]\n",
    "    \n",
    "    return output[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, n_tests, render=render):\n",
    "   scores = []\n",
    "   for _ in range(n_tests):\n",
    "     environment.reset()\n",
    "     observation = environment.reset()\n",
    "     reward_sum = 0\n",
    "     while True:\n",
    "        if render:\n",
    "            environment.render()\n",
    "\n",
    "        state = np.reshape(observation, [1, environment_dimension])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        observation, reward, done, _ = environment.step(action)\n",
    "        reward_sum += reward\n",
    "       \n",
    "        if done:\n",
    "           break\n",
    "        scores.append(reward_sum)\n",
    "        \n",
    "        environment.close()\n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_pole_game(environment, policy_model, model_predictions):\n",
    "    loss = []\n",
    "    n_episode, reward_sum, score, episode_done = 0, 0, 0, False\n",
    "    n_actions = environment.action_space.n\n",
    "    observation = environment.reset()\n",
    "    states = np.empty(0).reshape(0, environment_dimension)\n",
    "    actions = np.empty(0).reshape(0, 1)\n",
    "    rewards = np.empty(0).reshape(0, 1)\n",
    "    discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "\n",
    "    while n_episode < n_episodes:\n",
    "        state = np.reshape(observation, [1, environment_dimension])\n",
    "        prediction = model_predictions.predict([state])[0]\n",
    "        action = np.random.choice(range(environment.action_space.n), p=prediction)\n",
    "        states = np.vstack([states, state])\n",
    "        actions = np.vstack([actions, action])\n",
    "        observation, reward, episode_done, info = environment.step(action)\n",
    "        reward_sum += reward\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "\n",
    "        if episode_done == True:\n",
    "            discounted_reward = calculate_discounted_reward(rewards)\n",
    "            discounted_rewards = np.vstack([discounted_rewards, discounted_reward])\n",
    "            rewards = np.empty(0).reshape(0, 1)\n",
    "\n",
    "        if (n_episode + 1) % batch_size == 0:\n",
    "            discounted_rewards -= discounted_rewards.mean()\n",
    "            discounted_rewards /= discounted_rewards.std()\n",
    "            discounted_rewards = discounted_rewards.squeeze()\n",
    "            actions = actions.squeeze().astype(int)\n",
    "            train_actions = np.zeros([len(actions), n_actions])\n",
    "            train_actions[np.arange(len(actions)), actions] = 1\n",
    "            error = policy_model.train_on_batch([states, discounted_rewards], train_actions)\n",
    "            loss.append(error)\n",
    "            states = np.empty(0).reshape(0, environment_dimension)\n",
    "            actions = np.empty(0).reshape(0, 1)\n",
    "            discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "            score = score_model(model=model_predictions, n_tests=10)\n",
    "            print('\\nEpisode: %s \\nAverage Reward: %s \\nScore: %s \\nError: %s' %(n_episode+1, reward_sum/float(batch_size), score, np.mean(loss[-batch_size:])))\n",
    "\n",
    "        if score >= goal:\n",
    "            break\n",
    "\n",
    "        reward_sum = 0\n",
    "        n_episode += 1\n",
    "        observation = environment.reset()\n",
    "\n",
    "    plt.title('Policy Gradient Error plot over %s Episodes'%(n_episode+1))\n",
    "    plt.xlabel('N batches')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.plot(loss)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'input')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     mlp_model \u001b[39m=\u001b[39m PolicyGradient(\n\u001b[0;32m      3\u001b[0m         n_units\u001b[39m=\u001b[39mn_units,\n\u001b[0;32m      4\u001b[0m         n_layers\u001b[39m=\u001b[39mn_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         loss_function\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlog_likelihood\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m     )\n\u001b[1;32m---> 13\u001b[0m     policy_model, model_predictions \u001b[39m=\u001b[39m mlp_model\u001b[39m.\u001b[39;49mcreate_policy_model(environment_dimension,)\n\u001b[0;32m     15\u001b[0m     policy_model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     17\u001b[0m     cart_pole_game(\n\u001b[0;32m     18\u001b[0m         environment\u001b[39m=\u001b[39menvironment,\n\u001b[0;32m     19\u001b[0m         policy_model\u001b[39m=\u001b[39mpolicy_model,\n\u001b[0;32m     20\u001b[0m         model_predictions\u001b[39m=\u001b[39mmodel_predictions\n\u001b[0;32m     21\u001b[0m     )\n",
      "File \u001b[1;32mf:\\Fajri\\F\\Kuliah\\(S2) Program Magister\\Semester 2\\Advace Machine Learning\\Reinforcement Learning\\Reinforcment-Learning-Exercise\\neural_networks\\policy_gradient_utilities.py:53\u001b[0m, in \u001b[0;36mPolicyGradient.create_policy_model\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     51\u001b[0m policy_model \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39m[input_layer, advantages], outputs\u001b[39m=\u001b[39moutput_layer)\n\u001b[0;32m     52\u001b[0m policy_model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function, optimizer\u001b[39m=\u001b[39mAdam(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate))\n\u001b[1;32m---> 53\u001b[0m model_prediction \u001b[39m=\u001b[39m Model(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m[input_layer], outputs\u001b[39m=\u001b[39;49moutput_layer)\n\u001b[0;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m policy_model, model_prediction\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Fajri\\Anaconda\\envs\\gpu2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mf:\\Fajri\\Anaconda\\envs\\gpu2\\lib\\site-packages\\keras\\utils\\generic_utils.py:1269\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m   1268\u001b[0m     \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[1;32m-> 1269\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'input')"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mlp_model = PolicyGradient(\n",
    "        n_units=n_units,\n",
    "        n_layers=n_layers,\n",
    "        n_columns=environment_dimension,\n",
    "        n_outputs=n_classes,\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_activation='selu',\n",
    "        output_activation='softmax',\n",
    "        loss_function='log_likelihood'\n",
    "    )\n",
    "    \n",
    "    policy_model, model_predictions = mlp_model.create_policy_model(environment_dimension,)\n",
    "    \n",
    "    policy_model.summary()\n",
    "    \n",
    "    cart_pole_game(\n",
    "        environment=environment,\n",
    "        policy_model=policy_model,\n",
    "        model_predictions=model_predictions\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Munti Carlo**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
