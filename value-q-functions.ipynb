{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb7fb1d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-11T16:38:11.258121Z",
     "iopub.status.busy": "2023-06-11T16:38:11.257759Z",
     "iopub.status.idle": "2023-06-11T16:38:11.508013Z",
     "shell.execute_reply": "2023-06-11T16:38:11.506956Z"
    },
    "papermill": {
     "duration": 0.256191,
     "end_time": "2023-06-11T16:38:11.510072",
     "exception": false,
     "start_time": "2023-06-11T16:38:11.253881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df47afd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T16:38:11.515505Z",
     "iopub.status.busy": "2023-06-11T16:38:11.515161Z",
     "iopub.status.idle": "2023-06-11T16:38:11.523381Z",
     "shell.execute_reply": "2023-06-11T16:38:11.522258Z"
    },
    "papermill": {
     "duration": 0.012782,
     "end_time": "2023-06-11T16:38:11.525047",
     "exception": false,
     "start_time": "2023-06-11T16:38:11.512265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def value_function(env, gamma, epsilon):\n",
    "    \"\"\"\n",
    "    Calculates the state-value function for a given environment using the \n",
    "    iterative policy evaluation method.\n",
    "    \n",
    "    Args:\n",
    "    - env: The environment for which the state-value function is to be calculated.\n",
    "    - gamma: The discount factor for future rewards.\n",
    "    - epsilon: The threshold for determining the convergence of the state-value function.\n",
    "    - num_states: The number of states in the environment.\n",
    "    \n",
    "    Returns:\n",
    "    - V: A numpy array containing the state-value function values for each state in the environment.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0 \n",
    "        for state in range(env.observation_space.n):\n",
    "            old_value = V[state]\n",
    "            action_values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                next_state_values = []\n",
    "                for transition in env.P[state][action]: #env.P[state,action] => Transition Probability Function\n",
    "                    probability, next_state, reward, done = transition\n",
    "                    next_state_values.append(probability * (reward + gamma * V[next_state]))\n",
    "                action_values.append(sum(next_state_values))\n",
    "            V[state] = max(action_values)\n",
    "\n",
    "            delta = max(delta, abs(old_value - V[state]))\n",
    "        # for state in range(num_states):\n",
    "        #     print(f\"State {state}: Value = {V[state]}\")\n",
    "        # print(\"#\"*30)\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f633532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T16:38:11.530246Z",
     "iopub.status.busy": "2023-06-11T16:38:11.529869Z",
     "iopub.status.idle": "2023-06-11T16:38:11.537323Z",
     "shell.execute_reply": "2023-06-11T16:38:11.536545Z"
    },
    "papermill": {
     "duration": 0.011927,
     "end_time": "2023-06-11T16:38:11.538975",
     "exception": false,
     "start_time": "2023-06-11T16:38:11.527048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Q_function(env,alpha=0.1,gamma=0.9,epsilon_greedy=1,nb_episodes=100):\n",
    "    \"\"\"\n",
    "    This function implements the Q-learning algorithm for the given environment. It updates the Q-table based on the\n",
    "    rewards and actions taken, and returns the final Q-table and rewards for each episode. The function takes in the \n",
    "    following parameters:\n",
    "    \n",
    "    * env - The OpenAI gym environment.\n",
    "    * alpha - The learning rate. Default is 0.1.\n",
    "    * gamma - The discount factor. Default is 0.9.\n",
    "    * epsilon_greedy - The exploration-exploitation trade-off parameter. Default is 1.\n",
    "    * nb_episodes - The number of episodes to run. Default is 100.\n",
    "    \n",
    "    The function returns the following:\n",
    "    \n",
    "    * Q - The final Q-table after training.\n",
    "    * rewards - An array of rewards obtained for each episode.\n",
    "    \"\"\"\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    rewards = np.zeros(nb_episodes)\n",
    "    final_epsilon = 0.1\n",
    "    epsilon_decay = 0.9\n",
    "    for episode in range(nb_episodes):\n",
    "        state = env.reset()[0]\n",
    "        while True:\n",
    "            #epsilon greedy (Exploration/Exploitation)\n",
    "            if np.random.uniform(0,1)< epsilon_greedy:\n",
    "                action = env.action_space.sample() \n",
    "            else:\n",
    "                action = np.argmax(Q[state, :])\n",
    "            \n",
    "            if epsilon_greedy > final_epsilon:\n",
    "                epsilon_greedy *= epsilon_decay\n",
    "        \n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            Q[state,action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "            rewards[episode] += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    return Q,rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd2160e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T16:38:11.543502Z",
     "iopub.status.busy": "2023-06-11T16:38:11.543226Z",
     "iopub.status.idle": "2023-06-11T16:38:11.559794Z",
     "shell.execute_reply": "2023-06-11T16:38:11.558345Z"
    },
    "papermill": {
     "duration": 0.02173,
     "end_time": "2023-06-11T16:38:11.562488",
     "exception": false,
     "start_time": "2023-06-11T16:38:11.540758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: Value = 0.5904900000000002\n",
      "State 1: Value = 0.6561000000000001\n",
      "State 2: Value = 0.7290000000000001\n",
      "State 3: Value = 0.6561000000000001\n",
      "State 4: Value = 0.6561000000000001\n",
      "State 5: Value = 0.0\n",
      "State 6: Value = 0.81\n",
      "State 7: Value = 0.0\n",
      "State 8: Value = 0.7290000000000001\n",
      "State 9: Value = 0.81\n",
      "State 10: Value = 0.9\n",
      "State 11: Value = 0.0\n",
      "State 12: Value = 0.0\n",
      "State 13: Value = 0.9\n",
      "State 14: Value = 1.0\n",
      "State 15: Value = 0.0\n",
      "(4, 0.0, False, False, {'prob': 1.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1',is_slippery=False) #Stochastic=>is_slippery=True, Deterministic=>is_slippery=False\n",
    "env.reset()\n",
    "env.render()\n",
    "gamma = 0.9\n",
    "epsilon = 1e-6\n",
    "num_states = env.observation_space.n\n",
    "V = value_function(env, gamma, epsilon)\n",
    "for state in range(num_states):\n",
    "    print(f\"State {state}: Value = {V[state]}\")\n",
    "print(env.step(env.action_space.sample()))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a32e0f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T16:38:11.568543Z",
     "iopub.status.busy": "2023-06-11T16:38:11.568152Z",
     "iopub.status.idle": "2023-06-11T16:42:17.352973Z",
     "shell.execute_reply": "2023-06-11T16:42:17.352138Z"
    },
    "papermill": {
     "duration": 245.790905,
     "end_time": "2023-06-11T16:42:17.355762",
     "exception": false,
     "start_time": "2023-06-11T16:38:11.564857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100000\n",
    "Q,rewards = Q_function(env,nb_episodes=num_episodes)\n",
    "total_rewards = np.sum(rewards)\n",
    "print(f\"Total reward: {total_rewards}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 254.670502,
   "end_time": "2023-06-11T16:42:18.076726",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-11T16:38:03.406224",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
