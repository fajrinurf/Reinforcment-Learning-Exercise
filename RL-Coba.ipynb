{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning: Pengenalan**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di dalam Tutorial ini, akan menjelaskan penggunaan Reinforcement Learning dasar yang akan digunakan, dan juga untuk memenuhi tugas kuliah Advance Machine Learning. Referensi yang akan digunakan di dalam tutorial ini akan berbasis dari buku dan juga paper. untuk kasus yang akan dijelaskan disini adalah penggunaan Reinforcement Learning yang akan menerapkan metode Q-learning tabular klasik untuk [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) klasik Puzzle. \n",
    "\n",
    "![alt text](https://media2.giphy.com/media/46ib09ZL1SdWuREnj3/giphy.gif?cid=3640f6095c6e92762f3446634d90bc65) ![alt text](https://media0.giphy.com/media/d9QiBcfzg64Io/200w.webp?cid=3640f6095c6e93e92f30655873731752)![alt text](https://i.gifer.com/GpAY.gif)\n",
    "\n",
    "Reinforcement Learning bisa beroperasi dengan melakukan indentifikasi pola yang akan digunakan secara optimal, di dalam konteks dari masalah masalah yang diberikan, sehingga agen pada reinforcement learning dapat membuat keputusan terbaik untuk langkah berikutnya."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q-Learning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Fajri\\Anaconda\\envs\\gpu2\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "environment.reset()\n",
    "environment.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table =\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Our table has the following dimensions:\n",
    "# (rows x columns) = (states x actions) = (16 x 4)\n",
    "qtable = np.zeros((16, 4))\n",
    "\n",
    "# Alternatively, the gym library can also directly g\n",
    "# give us the number of states and actions using \n",
    "# \"env.observation_space.n\" and \"env.action_space.n\"\n",
    "nb_states = environment.observation_space.n  # = 16\n",
    "nb_actions = environment.action_space.n      # = 4\n",
    "qtable = np.zeros((nb_states, nb_actions))\n",
    "\n",
    "# Let's see how it looks\n",
    "print('Q-table =')\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RIGHT'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice([\"LEFT\", \"DOWN\", \"RIGHT\", \"UP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.step(2)\n",
    "environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Fajri\\Anaconda\\envs\\gpu2\\lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "action = environment.action_space.sample()\n",
    "\n",
    "# 2. Implement this action and move the agent in the desired direction\n",
    "new_state, reward, done, info, _ = environment.step(action)\n",
    "\n",
    "# Display the results (reward and map)\n",
    "environment.render()\n",
    "print(f'Reward = {reward}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m new_state_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(new_state_list)\n\u001b[0;32m     46\u001b[0m max_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(new_state_array)\n\u001b[1;32m---> 47\u001b[0m qtable[state, action] \u001b[39m=\u001b[39m qtable[state, action] \u001b[39m+\u001b[39m alpha \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m qtable[max_index] \u001b[39m-\u001b[39m qtable[state, action])\n\u001b[0;32m     48\u001b[0m \u001b[39m# qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable.item(new_state_array)) - qtable[state, action])\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[39m# Update our current state\u001b[39;00m\n\u001b[0;32m     51\u001b[0m state \u001b[39m=\u001b[39m new_state\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "\n",
    "# We re-initialize the Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 1000        # Total number of episodes\n",
    "alpha = 0.5            # Learning rate\n",
    "gamma = 0.9            # Discount factor\n",
    "\n",
    "# List of outcomes to plot\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Training\n",
    "for _ in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # By default, we consider our outcome to be a failure\n",
    "    outcomes.append(\"Failure\")\n",
    "    state_int = np.array(state)\n",
    "\n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        if np.max(qtable[state_int[0]]) > 0:\n",
    "          action = np.argmax(qtable[state_int[0]])\n",
    "\n",
    "\n",
    "        # If there's no best action (only zeros), take a random one\n",
    "        else:\n",
    "          action = environment.action_space.sample()\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, info, _ = environment.step(action)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        new_state_int = np.array(new_state)\n",
    "        new_state_list = new_state_int.tolist()\n",
    "        new_state_array = np.array(new_state_list)\n",
    "        max_index = np.argmax(new_state_array)\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * qtable[max_index] - qtable[state, action])\n",
    "        # qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable.item(new_state_array)) - qtable[state, action])\n",
    "        \n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # If we have a reward, it means that our outcome is a success\n",
    "        if reward:\n",
    "          outcomes[-1] = \"Success\"\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# Plot outcomes\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#efeeea')\n",
    "plt.bar(range(len(outcomes)), outcomes, color=\"#0A047A\", width=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Until the agent gets stuck or reaches the goal, keep training it\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Choose the action with the highest value in the current state\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmax(qtable[state]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m       action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(qtable[state])\n\u001b[0;32m     15\u001b[0m     \u001b[39m# If there's no best action (only zeros), take a random one\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluation\n",
    "for _ in range(100):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        if np.max(qtable[state]) > 0:\n",
    "          action = np.argmax(qtable[state])\n",
    "\n",
    "        # If there's no best action (only zeros), take a random one\n",
    "        else:\n",
    "          action = environment.action_space.sample()\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, info, _ = environment.step(action)\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # When we get a reward, it means we solved the game\n",
    "        nb_success += reward\n",
    "\n",
    "# Let's check our success rate!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m sequence \u001b[39m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Choose the action with the highest value in the current state\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmax(qtable[state]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     11\u001b[0m       action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(qtable[state])\n\u001b[0;32m     13\u001b[0m     \u001b[39m# If there's no best action (only zeros), take a random one\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time \n",
    "\n",
    "state = environment.reset()\n",
    "done = False\n",
    "sequence = []\n",
    "\n",
    "while not done:\n",
    "    # Choose the action with the highest value in the current state\n",
    "    if np.max(qtable[state]) > 0:\n",
    "      action = np.argmax(qtable[state])\n",
    "\n",
    "    # If there's no best action (only zeros), take a random one\n",
    "    else:\n",
    "      action = environment.action_space.sample()\n",
    "    \n",
    "    # Add the action to the sequence\n",
    "    sequence.append(action)\n",
    "\n",
    "    # Implement this action and move the agent in the desired direction\n",
    "    new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "    # Update our current state\n",
    "    state = new_state\n",
    "\n",
    "    # Update the render\n",
    "    clear_output(wait=True)\n",
    "    environment.render()\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Sequence = {sequence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon GReady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m new_state, reward, done, info, _ \u001b[39m=\u001b[39m environment\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     39\u001b[0m \u001b[39m# Update Q(s,a)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m qtable[state, action] \u001b[39m=\u001b[39m qtable[state, action] \u001b[39m+\u001b[39m \\\n\u001b[0;32m     41\u001b[0m                         alpha \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(qtable[new_state]) \u001b[39m-\u001b[39m qtable[state, action])\n\u001b[0;32m     43\u001b[0m \u001b[39m# Update our current state\u001b[39;00m\n\u001b[0;32m     44\u001b[0m state \u001b[39m=\u001b[39m new_state\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 1000        # Total number of episodes\n",
    "alpha = 0.5            # Learning rate\n",
    "gamma = 0.9            # Discount factor\n",
    "epsilon = 1.0          # Amount of randomness in the action selection\n",
    "epsilon_decay = 0.001  # Fixed amount to decrease\n",
    "\n",
    "# List of outcomes to plot\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Training\n",
    "for _ in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # By default, we consider our outcome to be a failure\n",
    "    outcomes.append(\"Failure\")\n",
    "    \n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Generate a random number between 0 and 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # If random number < epsilon, take a random action\n",
    "        if rnd < epsilon:\n",
    "          action = environment.action_space.sample()\n",
    "        # Else, take the action with the highest value in the current state\n",
    "        else:\n",
    "          action = np.argmax(qtable[state])\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, info, _ = environment.step(action)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "        \n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # If we have a reward, it means that our outcome is a success\n",
    "        if reward:\n",
    "          outcomes[-1] = \"Success\"\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# Plot outcomes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#efeeea')\n",
    "plt.bar(range(len(outcomes)), outcomes, color=\"#0A047A\", width=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Until the agent gets stuck or reaches the goal, keep training it\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Choose the action with the highest value in the current state\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(qtable[state])\n\u001b[0;32m     14\u001b[0m     \u001b[39m# Implement this action and move the agent in the desired direction\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     new_state, reward, done, info, _ \u001b[39m=\u001b[39m environment\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluation\n",
    "for _ in range(100):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, info, _ = environment.step(action)\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # When we get a reward, it means we solved the game\n",
    "        nb_success += reward\n",
    "\n",
    "# Let's check our success rate!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slipery Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m new_state, reward, done, info, _ \u001b[39m=\u001b[39m environment\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     43\u001b[0m \u001b[39m# Update Q(s,a)\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m qtable[state, action] \u001b[39m=\u001b[39m qtable[state, action] \u001b[39m+\u001b[39m \\\n\u001b[0;32m     45\u001b[0m                         alpha \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(qtable[new_state]) \u001b[39m-\u001b[39m qtable[state, action])\n\u001b[0;32m     47\u001b[0m \u001b[39m# Update our current state\u001b[39;00m\n\u001b[0;32m     48\u001b[0m state \u001b[39m=\u001b[39m new_state\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "environment.reset()\n",
    "\n",
    "# We re-initialize the Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 1000        # Total number of episodes\n",
    "alpha = 0.5            # Learning rate\n",
    "gamma = 0.9            # Discount factor\n",
    "epsilon = 1.0          # Amount of randomness in the action selection\n",
    "epsilon_decay = 0.001  # Fixed amount to decrease\n",
    "\n",
    "# List of outcomes to plot\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Training\n",
    "for _ in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # By default, we consider our outcome to be a failure\n",
    "    outcomes.append(\"Failure\")\n",
    "    \n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Generate a random number between 0 and 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # If random number < epsilon, take a random action\n",
    "        if rnd < epsilon:\n",
    "          action = environment.action_space.sample()\n",
    "        # Else, take the action with the highest value in the current state\n",
    "        else:\n",
    "          action = np.argmax(qtable[state])\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, info, _ = environment.step(action)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "        \n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # If we have a reward, it means that our outcome is a success\n",
    "        if reward:\n",
    "          outcomes[-1] = \"Success\"\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# Plot outcomes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#efeeea')\n",
    "plt.bar(range(len(outcomes)), outcomes, color=\"#0A047A\", width=1.0)\n",
    "plt.show()\n",
    "\n",
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluation\n",
    "for _ in range(100):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # When we get a reward, it means we solved the game\n",
    "        nb_success += reward\n",
    "\n",
    "# Let's check our success rate!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "import itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.monitor_interval = 0\n",
    "\n",
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space.n):\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n):\n",
    "            p[action] = 1 / env.action_space.n\n",
    "        policy[key] = p\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "        Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(env, policy, display=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "\n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "\n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "            top_range += prob[1]\n",
    "            if n < top_range:\n",
    "                action = prob[0]\n",
    "                break \n",
    "        s_prime, reward, finished, info, _= env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "\n",
    "        episode.append(timestep)\n",
    "\n",
    "        s = s_prime  # update the state\n",
    "\n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(1)\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, env):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w = run_game(env, policy, display=False)[-1][-1]\n",
    "        if w == 1:\n",
    "            wins += 1\n",
    "    return wins / r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
    "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {} # 3.\n",
    "    \n",
    "    for _ in range(episodes): # Looping through episodes\n",
    "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
    "        \n",
    "        # for loop through reversed indices of episode array. \n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "        \n",
    "        for i in reversed(range(0, len(episode))):   \n",
    "            s_t, a_t, r_t = episode[i] \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q # 14.\n",
    "                \n",
    "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = monte_carlo_e_soft(env, episodes=5000)\n",
    "test_policy(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
